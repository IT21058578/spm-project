{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from scipy.stats import zscore\n",
    "from kneed import KneeLocator\n",
    "from feature_engine import transformation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data, true = make_blobs(\n",
    "    n_samples=1000, \n",
    "    n_features=60,\n",
    "    centers=5,\n",
    "    shuffle=True,\n",
    "    center_box=(0, 5),\n",
    "    cluster_std=4,\n",
    "    random_state=1\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(data);\n",
    "\n",
    "def transform_column(col=pd.Series()) -> pd.Series:\n",
    "    min = col.min()\n",
    "    col = col.apply(lambda x : x - min)\n",
    "    return col\n",
    "\n",
    "# Form data to format we want\n",
    "df = df.round(0)\n",
    "df = df.apply(lambda x : transform_column(x))\n",
    "\n",
    "# Removing outliers\n",
    "org_df = df.copy()\n",
    "df = df[(np.abs(zscore(df)) <= 3).all(axis=1)]\n",
    "temp = pd.merge(org_df, df, how=\"outer\", suffixes=(\"\", \"_y\"), indicator=True)\n",
    "temp = temp[temp['_merge']=='left_only'][org_df.columns].index.array\n",
    "true = np.delete(true, temp)\n",
    "\n",
    "# Yeojohnson transformation for each column\n",
    "transformer = transformation.YeoJohnsonTransformer()\n",
    "df = transformer.fit_transform(df)\n",
    "\n",
    "# Standard scaling for all values\n",
    "scaler = StandardScaler()\n",
    "df = pd.DataFrame(scaler.fit_transform(df))\n",
    "\n",
    "kmeans_kwargs = {\n",
    "    \"init\": \"k-means++\",\n",
    "    \"n_init\": 10,\n",
    "    \"max_iter\": 300,\n",
    "    \"n_clusters\": 5,\n",
    "}\n",
    "\n",
    "# Doing the clustering\n",
    "kmeans = KMeans(**kmeans_kwargs)\n",
    "res = kmeans.fit(df).labels_\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "clust_cnt = kmeans_kwargs['n_clusters']\n",
    "clust_tag_avgs = []\n",
    "for i in range(0, clust_cnt):\n",
    "    idxs = np.where(res == i)[0]\n",
    "    usr_count = len(idxs)\n",
    "    tot_usr_count = len(res)\n",
    "    # If we use the original df (raw counts) it becomes skewed \n",
    "    tag_avgs = df.iloc[idxs].sum().apply(lambda x: x / usr_count)\n",
    "    clust_tag_avgs.append(tag_avgs)\n",
    "\n",
    "clust_tag_weights = []\n",
    "for i in range(0, clust_cnt):\n",
    "    min = np.min(clust_tag_avgs[i])\n",
    "    temp = clust_tag_avgs[i].apply(lambda x: x - min)\n",
    "    sum = temp.sum()\n",
    "    temp = temp.apply(lambda x: x / sum)\n",
    "    clust_tag_weights.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from feature_engine.transformation import YeoJohnsonTransformer\n",
    "from scipy.stats import zscore\n",
    "\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "\n",
    "preprocessor = Pipeline(\n",
    "    [\n",
    "        (\"transformer\", YeoJohnsonTransformer()),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "kmeans = Pipeline(\n",
    "    [\n",
    "        (\"kmeans\", \n",
    "            KMeans(\n",
    "                init=\"k-means++\",\n",
    "                n_init=10,\n",
    "                max_iter=300,\n",
    "                n_clusters=5,\n",
    "            ) \n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"clusterer\", kmeans)\n",
    "    ]\n",
    ")\n",
    "\n",
    "data, true = make_blobs(\n",
    "    n_samples=1000, \n",
    "    n_features=61,\n",
    "    centers=5,\n",
    "    shuffle=True,\n",
    "    center_box=(0, 5),\n",
    "    cluster_std=4,\n",
    "    random_state=1\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(data);\n",
    "\n",
    "def transform_column(col=pd.Series()) -> pd.Series:\n",
    "    min = col.min()\n",
    "    col = col.apply(lambda x : x - min)\n",
    "    return col\n",
    "\n",
    "# Form data to format we want\n",
    "df = df.round(0)\n",
    "df = df.apply(lambda x : transform_column(x))\n",
    "\n",
    "# Removing outliers\n",
    "df = df[(np.abs(zscore(df)) <= 3).all(axis=1)]\n",
    "\n",
    "# Prepare it. Here DF is a df of users against their total tag counts\n",
    "pipeline.fit(df)\n",
    "\n",
    "# Save it to a file\n",
    "file = open('model.pkl', \"wb\")\n",
    "pkl.dump(pipeline, file)\n",
    "file.close()\n",
    "\n",
    "# Get important additional data\n",
    "labels = pipeline['clusterer']['kmeans'].labels_\n",
    "clust_tag_avgs = []\n",
    "for i in range(0, 5):\n",
    "    idxs = np.where(labels == i)[0]\n",
    "    usr_count = len(idxs)\n",
    "    tot_usr_count = len(labels)\n",
    "    # If we use the original df (raw counts) it becomes skewed \n",
    "    tag_avgs = df.iloc[idxs].sum().apply(lambda x: x / usr_count)\n",
    "    clust_tag_avgs.append(tag_avgs)\n",
    "\n",
    "clust_tag_weights = []\n",
    "for i in range(0, 5):\n",
    "    min = np.min(clust_tag_avgs[i])\n",
    "    temp = clust_tag_avgs[i].apply(lambda x: x - min)\n",
    "    sum = temp.sum()\n",
    "    temp = temp.apply(lambda x: x / sum)\n",
    "    clust_tag_weights.append(temp)\n",
    "\n",
    "clust_data = {\n",
    "    'cluster_avgs_': clust_tag_avgs,\n",
    "    'cluster_weights_': clust_tag_weights\n",
    "}\n",
    "\n",
    "# Save important additional data\n",
    "file = open('data.pkl', 'wb')\n",
    "pkl.dump(clust_data, file)\n",
    "file.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
